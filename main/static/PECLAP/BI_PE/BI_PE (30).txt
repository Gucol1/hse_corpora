Introduction
Background. Store and process system development containing data about a country's physical addresses could allow companies that process or use this type of data to avoid delays and a "bottleneck effect" while working with storage.
A fast data access interface and ways to efficiently store and allocate resources can be automated using modern development approaches. With the development of cloud computing, distributed systems of data storage and processing, algorithms of parallel processing and distribution of computational nodes, which are based on the division of the task and the subsequent performance of several simple calculations on several machines, are qualitatively and quantitatively developed. Thus, the complexity of processing large data can be reduced by dividing the whole set into parts or by using a different representation of data in the machine memory. Based on experimental studies, the effectiveness of some tools and mechanisms has already been proved, but not yet completed.
Parma TG, a software company developing software for the automation of state documentation in Russia, was tasked with storage and quick access to address and real estate data in the Russian Federation. The data is stored in the state registry "FIAS" and is required for requests in most parts of the system. Thus, this product can be used to speed up and simplify access to data, the calculation and processing of which on the current solution stack is not possible or does not meet the speed and memory requirements. Besides, in the future, not only company developers will use this product, but also the customer side, so it is important to develop a system that guarantees high fault tolerance, responsiveness, and can be scaled and transferred to different computing nodes.
The address storage module should not only keep data up to date but also provide a quick way to obtain and aggregate it. The volume of information is large, so an efficient algorithm of storage and retrieval is required. One of the important solutions to increase the speed is the way to store - columns databases, which reduces the load when selecting data. It is expected that the application under development will be included in the company's application system. The object of the project is to address data storage, the object is data indexing in column DBMS.
Problem Statement. The work purpose is the analysis and development of a solution that allows using effectively a method of caching data for reduction time of access and minimization of computing resources, for storage and processing data from the register of address storage "FIAS".
For correct choice of the tool, it is necessary, first to understand how exactly it is necessary to process and store data, and second what restrictions there are at work (on time, memory, etc.). In this case, it is necessary to store data on all real estate objects in the Russian Federation and to use the tool which can quickly (not slower than 0,3 seconds) carry out any coming inquiry.
Professional Significance. The theoretical importance of this work is to study address storage models and a new application of tools based on column DBMS. It is of practical importance to create a storage and data processing module for data processing and its subsequent implementation in the company.
Delimitation of studies. This paper describes the existing approaches: cached dictionary, relational and NoSQL databases, as well as column storages. 
However, all examples except the last have already been tested and have not confirmed the profitability of their use. Data caching is very fast but requires a lot of RAM, so alternative methods of quick access are also considered. The relational database is too slow, whereas, the NoSQL database does not fit into the infrastructure of the current application. 
Summarized the foregoing, it was decided to use a column database ClickHouse from the Yandex company. This product was recently introduced to the market, is relevant and actively used by large IT-companies and Yandex itself for quick access to big data. Therefore, cases when ClickHouse should be used, pitfalls of the given technology, and a way of application of the bar chart directly on the project are considered below.
Literature review
During storing and processing a large amount of information, problems arise. These problems are quite topical and need various solutions. Therefore, it is necessary to consider existing solutions and approaches to the problems solving, before a solution for the Federal Informational Address System (FIAS) will be developed. The literature review provides the significance of the problems under consideration as well as some existing approaches and possible solutions. Data in FIAS is highly structured, thus it is necessary to consider approaches to processing and storing large amounts of SQL-like tabular data. Besides, it is necessary to consider the current architecture of the application where the developed system will be embedded, choose the optimal solution tool that does not change the interaction of existing components. Thirdly, it is necessary to investigate the existing solutions for the problem to be solved to find the best way to figure it. 
Efficient online information service decreases baseless data duplication and ultimately it reduces cost and time required for the development. The importance and requirements for the ability to process large data at high speed are still increasing. It is predicted that big data analysis will grow rapidly and so intensely that by 2020 about 50% of all developed software will include big data analysis. Furthermore, IT companies' and integrators' annual achievements reports show the interest of the business in the use and creation of such solutions. According to, almost 90% of B&B companies tend to use own solutions in big data processing or outsource it.
There are technologies for storing and processing a large amount of information. Furthermore, there is a cutting-edge technology that the big variety of NoSQL approaches becomes more common. NoSQL database does not require prior structuring of stored data, as do SQL-storage. NoSQL tools interact with unallocated information, which makes it possible to process complex structures, but often loses in processing speed, due to the lack of mechanisms that work equally well on different storage formats. Tabular storages impose restrictions on the storage format but instead provides fast and clear access to date, due to a pre-formed interaction model. The data in "FIAS" registry is tabular, thus it makes sense to use SQL-like tools. However, it is necessary to make sure that exemplary applications - representatives of the NoSQL family do not cope with the task solution or cope worse than tabular tools for some reason.
In the current application ecosystem, there are already representatives of different approaches to data processing: Transaction approach (OLTP), which implies fast processing and aggregation of small parts of information into one large group. Furthermore, an analytic approach (OLAP) to processing data that stored and processed in a complex structured object. The company also works with fast data access systems, such as Key-Value dictionaries and caching systems. Earlier employees in the company tried to use all approaches described above, but all the attempts did not give the required result.  
It is noticeable that the OLAP work scenario differs significantly from other common work scenarios (for example, OLTP or Key-Value). Thus, it is not necessary to try to use OLTP or Key-Value DB to process analytical queries for high performance. The article describes attempts to use tabular solutions, but they all failed. Storage with OLTP approach allows to work with data successfully, but it takes a long time to get the result. In system of quick access for data storage is offered but to use the offered solution in the existing architecture of the company's applications not less than 300 Gb of RAM is required, which is very expensive. As solutions work with the database manager in memory and all the processing takes place in RAM. The approach described in is inconvenient to use, does not fit into the architecture of the existing system and similarly to tabular tools does not meet the requirements to the processing speed.
The greater the load on a system, the more important the specialization in a work scenario becomes, and the more specific this specialization becomes. There is no system that is equally suitable for significantly different types of specialization.
In article offers the use of views - copies of data that change less frequently than the main storage. It assumes relatively infrequent requests (less than 100 times per second to the server), data loading on a schedule, so rare data changes and most requests - for reading. Also, the result of the query is expected to be significantly less than the original data - that is, the data is filtered or aggregated. Thus, based on the requirements of scripts, environment infrastructure, and code openness requirements, the decision was made to use the representation as a showcase of address data.
In Yandex's article about their experience of solving the task of fast processing of large data arrays was mentioned using their analytical tool ClickHouse. It is a column database system for online executing analytical requests (OLAP). The processing benchmark in shows that the column DBMS is better (from 100 times the processing speed of most queries) for an OLAP scenario. Besides, since the data is read in batches, it is easier to compress it. The data on the columns is also better compressed.
Since it is necessary to process a sufficiently large number of rows execute to the query, it becomes relevant to dispatch all operations, not for individual rows, but entire vectors, or to implement the query execution engine so that the costs of dispatching are approximately zero. If this is not done, then in any disk subsystem the query interpreter will load the CPU up to 100%. It makes sense not only to store data by columns but also to process them, if possible, also by columns.
According to the authors of, there are two ways to do it:
1. The vector engine. All operations are written not for individual values, but vectors. That is, it is necessary to call operations quite rarely, and the costs of dispatching become negligible. 
2. The code generation. Code is generated for the request, in which all indirect calls are substituted.
In SQL databases, this is not done, as it does not make sense when performing simple queries. Although there are exceptions. It is worth noting that for CPU efficiency it is required that the query language be declarative (SQL, MDX) or at least vector (J, K). That is, the query should contain loops only implicitly, opening opportunities for optimization.
Thus, it is reasonable to use the columnar database with additional indexing engines while developing the system.
Methods
During the research, object-oriented design and programming will be used to develop system storage. The software under development will be modelled using UML diagrams, namely: class diagrams to represent the subject area, and component diagrams to model the software architecture. For development, it is planned to use ClickHouse DBMS and object-oriented high-level programming languages C#, Go, Kotlin, as well as to compare the processing speed of each of the used database clients.
Methods of work with column databases will help in implementing a multidimensional information model for indexing based on addressing estate. There are several table engines in ClickHouse that are used for indexing methods. The most versatile and functional table engines for tasks with high load is Merge Tree index. A common feature of these engines is fast data insertion with subsequent background data processing. MergeTree engines support data replication, partification, and other features not supported for other engines, which makes this method of addressing the best for use in address processing.
The multi-level set is represented as a special tree-like multidimensional structure, where each node contains an array of pointers to each subsequent record. Pointers may contain either stored data or a reference to another tree by the numerical code of the corresponding element. One of the differences of this structure is that the trees are intended for storing information in RAM, but for implementing addressing we offer a similar structure for storing it directly in ClickHouse DBMS.
Results anticipated
The result of the study should be the storage of addresses from the FIAS registry, allowing to store and retrieve information about streets, houses and other elements of a real estate in Russia, with operating time shorter than the time of processing in a relational database. The information in the registry consists of a structured part with information about the buildings and data about the structure of their storage. The structured information is likely to be stored in a column database.
Huge efficiency of data storage is planned to be achieved by using a column database management system, one of the approaches that accelerate several operations, such as reading and aggregation, and implementation of addressing in the specified environment. Besides, it is assumed that a column DBMS can not only increase the information retrieval speed in documents but also reduce the memory size for storing indexes and facilitate horizontal expansion.
This notion is likely to be published and placed on a separate virtual machine in the company's cluster based on the Linux operating system mainly because the implementation tools have demonstrated their effectiveness in such a configuration.
 
